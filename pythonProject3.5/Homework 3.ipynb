{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ece1b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "divine-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11.2000, 27.1200, 42.9600, 56.4800, 68.6000, 79.4400, 88.4800, 92.8000,\n",
      "        89.2800, 73.4800, 20.6400])\n",
      "tensor([11.2000, 27.1200, 42.9600, 56.4800, 68.6000, 79.4400, 88.4800, 92.8000,\n",
      "        89.2800, 73.4800, 20.6400])\n"
     ]
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t \n",
    "#import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "A21 = (t.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0], requires_grad=True)) #\n",
    "#A12 = Variable(t.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0], requires_grad=True)) #\n",
    "P = [28.1, 34.3, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5] #P values \n",
    "\n",
    "X1 = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.1, 1.2]#X1 values\n",
    "\n",
    "\n",
    "# Define a loss\n",
    "\n",
    "for i in range(11) : #loss = loss + i\n",
    "   \n",
    "    loss = -(X1[i]*A21[i] - P[i])**2 \n",
    "    totloss = loss + loss\n",
    "    totloss.backward()\n",
    "print(A21.grad)\n",
    "\n",
    "\n",
    "A12 = (t.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0], requires_grad=True)) #\n",
    "#A12 = Variable(t.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0], requires_grad=True)) #\n",
    "P = [28.1, 34.3, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5] #P values \n",
    "\n",
    "X1 = [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.1, 1.2]#X1 values\n",
    "\n",
    "\n",
    "# Define a loss\n",
    "\n",
    "for i in range(11) : #loss = loss + i\n",
    "   \n",
    "    loss = -(X1[i]*A12[i] - P[i])**2 \n",
    "    totloss = loss + loss\n",
    "    totloss.backward()\n",
    "print(A12.grad)\n",
    "# Take gradient\n",
    "\n",
    "# pnew = X1*math.exp(A12*(((A21*(1- value for value in X1))/((A12*X1)*(A21*(1-value for value in X1))))**2))*(119.558)+(1-value for value in X1)*math.exp(A21*(((A12*(1-value for value in X1))/((A12*X1)*(A21*(1-value for value in X1))))**2))*(91.519)\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "#A21.grad.numpy()\n",
    "#print(A21.grad.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02adbf",
   "metadata": {},
   "source": [
    "The model does not fit the data too well there are lots of deviations between the data in the table and the data found through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f3492e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in c:\\users\\danny\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.14.0 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.20.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\danny\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
      "|   iter    |  target   |    X1     |    X2     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.265   \u001b[0m | \u001b[0m-0.4979  \u001b[0m | \u001b[0m 0.8813  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-110.1   \u001b[0m | \u001b[0m-2.999   \u001b[0m | \u001b[0m-0.7907  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.4933  \u001b[0m | \u001b[0m-0.3849  \u001b[0m | \u001b[0m 1.039   \u001b[0m |\n",
      "=================================================\n",
      "{'target': 0.2650082867644827, 'params': {'X1': -0.4978679717845562, 'X2': 0.8812979737686324}}\n",
      "|   iter    |  target   |    X1     |    X2     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.265   \u001b[0m | \u001b[0m-0.4979  \u001b[0m | \u001b[0m 0.8813  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 110.1   \u001b[0m | \u001b[95m-2.999   \u001b[0m | \u001b[95m-0.7907  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 71.79   \u001b[0m | \u001b[0m-2.842   \u001b[0m | \u001b[0m-0.6775  \u001b[0m |\n",
      "=================================================\n",
      "{'target': 110.13991869176739, 'params': {'X1': -2.9993137510959307, 'X2': -0.7906697094726409}}\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "def function(X1, X2):\n",
    "    return (-(4-2.1*X1**2+(X1**4)/3)*X1**2+(-X1*X2)+(-1)*(-4+4*X2**2)*X2**2)\n",
    "pbounds = {\n",
    "    'X1' : (-3, 3), \n",
    "    'X2' : (-2, 2)}\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=function,\n",
    "                                 pbounds=pbounds,\n",
    "                                 random_state=1)\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=1)\n",
    "\n",
    "print(optimizer.max) \n",
    "\n",
    "# Finding Max\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "def function(X1, X2):\n",
    "    return ((4-2.1*X1**2+(X1**4)/3)*X1**2+(X1*X2)+(-4+4*X2**2)*X2**2)\n",
    "pbounds = {\n",
    "    'X1' : (-3, 3), \n",
    "    'X2' : (-2, 2)}\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(f=function,\n",
    "                                 pbounds=pbounds,\n",
    "                                 random_state=1)\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=1)\n",
    "\n",
    "print(optimizer.max)\n",
    "#Finds minimum function by multiplying by -1 to invert of x axis finding min values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
